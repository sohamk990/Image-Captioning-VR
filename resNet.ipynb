{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport spacy\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport torchvision.transforms as T\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom PIL import Image\nprint(\"IMPORTED SUCCESSFULLY ðŸ˜Ž\")","metadata":{"_uuid":"fa187dfa-226b-4c11-bdf5-d8c7187722f5","_cell_guid":"324a2e16-dc65-40cb-a45f-32c9ca7f23d6","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-05-22T16:32:57.573076Z","iopub.execute_input":"2021-05-22T16:32:57.573464Z","iopub.status.idle":"2021-05-22T16:33:06.372777Z","shell.execute_reply.started":"2021-05-22T16:32:57.573384Z","shell.execute_reply":"2021-05-22T16:33:06.371350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image(img, title=None):\n    img[0] = img[0] * 0.229\n    img[1] = img[1] * 0.224 \n    img[2] = img[2] * 0.225 \n    img[0] += 0.485 \n    img[1] += 0.456 \n    img[2] += 0.406    \n    img = img.numpy().transpose((1, 2, 0))\n    plt.imshow(img)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:06.374248Z","iopub.execute_input":"2021-05-22T16:33:06.374577Z","iopub.status.idle":"2021-05-22T16:33:06.380940Z","shell.execute_reply.started":"2021-05-22T16:33:06.374542Z","shell.execute_reply":"2021-05-22T16:33:06.380051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self,freq_threshold):\n        #setting the pre-reserved tokens int to string tokens\n        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n        \n        #string to int tokens\n        #its reverse dict self.itos\n        self.stoi = {v:k for k,v in self.itos.items()}        \n        self.freq_threshold = freq_threshold\n        self.frequencies = Counter()\n        \n    def __len__(self):\n        return len(self.itos)\n    \n    @staticmethod\n    def tokenize(text):\n        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n    \n    def build_vocab(self, sentence_list):\n        idx = 4        \n        for sentence in sentence_list:\n            for word in self.tokenize(sentence):\n                self.frequencies[word] += 1\n                \n                #add the word to the vocab if it reaches minum frequecy threshold\n                if self.frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    \n    def numericalize(self,text):\n        tokenized_text = self.tokenize(text)\n        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]\n    \n    def word(self,ind):\n        return [self.itos[idx] if idx in self.itos else self.itos[\"<UNK\"] for idx in ind]","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:06.383102Z","iopub.execute_input":"2021-05-22T16:33:06.383453Z","iopub.status.idle":"2021-05-22T16:33:06.396099Z","shell.execute_reply.started":"2021-05-22T16:33:06.383418Z","shell.execute_reply":"2021-05-22T16:33:06.395301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    def __init__(self,root_dir,captions_file,transform=None,freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(caption_file)\n        self.transform = transform\n        \n        #Get image and caption colum from the dataframe\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        \n        #Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocab(self.captions.tolist())\n        \n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self,idx):\n        caption = self.captions[idx]\n        img_name = self.imgs[idx]\n        img_location = os.path.join(self.root_dir,img_name)\n        img = Image.open(img_location).convert(\"RGB\")\n        \n        #apply the transfromation to the image\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        #numericalize the caption text\n        caption_vec = []\n        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n        caption_vec += self.vocab.numericalize(caption)\n        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n        \n        return img, torch.tensor(caption_vec)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:06.398503Z","iopub.execute_input":"2021-05-22T16:33:06.398801Z","iopub.status.idle":"2021-05-22T16:33:06.409449Z","shell.execute_reply.started":"2021-05-22T16:33:06.398776Z","shell.execute_reply":"2021-05-22T16:33:06.408719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CapsCollate:\n    def __init__(self,pad_idx,batch_first=False):\n        self.pad_idx = pad_idx\n        self.batch_first = batch_first\n    \n    def __call__(self,batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs,dim=0)\n        \n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n        return imgs,targets","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:06.410660Z","iopub.execute_input":"2021-05-22T16:33:06.411022Z","iopub.status.idle":"2021-05-22T16:33:06.425478Z","shell.execute_reply.started":"2021-05-22T16:33:06.410986Z","shell.execute_reply":"2021-05-22T16:33:06.424543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initiate the Dataset and Dataloader\nspacy_eng = spacy.load(\"en\")\n\ncaption_file = '../input/flickr8k/captions.txt'\ndata_location =  \"../input/flickr8k\"\n\n#setting the constants\nBATCH_SIZE = 500\n# BATCH_SIZE = 6\nNUM_WORKER = 0\n\ntransforms = T.Compose([\n    T.Resize(226),                     \n    T.RandomCrop(224),                 \n    T.ToTensor(),                               \n    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n])\n\n\n#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"/Images\",\n    captions_file = data_location+\"/captions.txt\",\n    transform=transforms\n)\n\n#writing the dataloader\ndata_loader = DataLoader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n    # batch_first=False\n)\n\n#vocab_size\nvocab_size = len(dataset.vocab)\npad_idx = dataset.vocab.stoi[\"<PAD>\"]\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:06.435759Z","iopub.execute_input":"2021-05-22T16:33:06.436276Z","iopub.status.idle":"2021-05-22T16:33:09.424175Z","shell.execute_reply.started":"2021-05-22T16:33:06.436240Z","shell.execute_reply":"2021-05-22T16:33:09.423344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nBATCH_SIZE = 500\n\nNUM_WORKER = 0\n\n\ntransforms = T.Compose([T.Resize(256),T.RandomCrop(224),T.ToTensor(),T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))])\ndataset =  FlickrDataset(root_dir = '../input/flickr8k/Images',captions_file = '../input/flickr8k/captions.txt',transform=transforms)\ndata_loader = DataLoader(dataset=dataset,batch_size=BATCH_SIZE,num_workers=NUM_WORKER,shuffle=True,collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True))\nvocab_size = len(dataset.vocab)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:09.426892Z","iopub.execute_input":"2021-05-22T16:33:09.427277Z","iopub.status.idle":"2021-05-22T16:33:10.913536Z","shell.execute_reply.started":"2021-05-22T16:33:09.427244Z","shell.execute_reply":"2021-05-22T16:33:10.912679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet50(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)        \n\n    def forward(self, images):\n        features = self.resnet(images)                                    \n        features = features.permute(0, 2, 3, 1)                           \n        features = features.view(features.size(0), -1, features.size(-1)) \n        return features","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:10.915390Z","iopub.execute_input":"2021-05-22T16:33:10.915878Z","iopub.status.idle":"2021-05-22T16:33:10.923188Z","shell.execute_reply.started":"2021-05-22T16:33:10.915840Z","shell.execute_reply":"2021-05-22T16:33:10.922305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n        super(Attention, self).__init__()        \n        self.attention_dim = attention_dim        \n        self.W = nn.Linear(decoder_dim,attention_dim)\n        self.U = nn.Linear(encoder_dim,attention_dim)        \n        self.A = nn.Linear(attention_dim,1)\n        \n    def forward(self, features, hidden_state):\n        u_hs = self.U(features)     \n        w_ah = self.W(hidden_state) \n        \n        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) \n        \n        attention_scores = self.A(combined_states)         \n        attention_scores = attention_scores.squeeze(2)     \n        \n        \n        alpha = F.softmax(attention_scores,dim=1)          \n        \n        attention_weights = features * alpha.unsqueeze(2)  \n        attention_weights = attention_weights.sum(dim=1)   \n        \n        return alpha,attention_weights      ","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:10.924403Z","iopub.execute_input":"2021-05-22T16:33:10.924841Z","iopub.status.idle":"2021-05-22T16:33:10.939149Z","shell.execute_reply.started":"2021-05-22T16:33:10.924788Z","shell.execute_reply":"2021-05-22T16:33:10.938296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Attention Decoder\nclass DecoderRNN(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        \n        #save the model param\n        self.vocab_size = vocab_size\n        self.attention_dim = attention_dim\n        self.decoder_dim = decoder_dim\n        \n        self.embedding = nn.Embedding(vocab_size,embed_size)\n        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n        \n        self.init_h = nn.Linear(encoder_dim, decoder_dim)\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)\n        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        \n        self.fcn = nn.Linear(decoder_dim,vocab_size)\n        self.drop = nn.Dropout(drop_prob)\n    \n    def forward(self, features, captions):\n        \n        \n        embeds = self.embedding(captions)\n        \n        \n        h, c = self.init_hidden_state(features)  \n        \n        seq_length = len(captions[0])-1 \n        batch_size = captions.size(0)\n        num_features = features.size(1)\n        \n        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n                \n        for s in range(seq_length):\n            alpha,context = self.attention(features, h)\n            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n            output = self.fcn(self.drop(h))\n            preds[:,s] = output\n            alphas[:,s] = alpha\n        return preds, alphas\n\n    def generate_caption(self,features,max_len=20,vocab=None):\n        \n        batch_size = features.size(0)\n        h, c = self.init_hidden_state(features)  \n\n        alphas = []\n        \n        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n        embeds = self.embedding(word)       \n        captions = []\n        \n        for i in range(max_len):\n            alpha,context = self.attention(features, h)\n            \n            \n            alphas.append(alpha.cpu().detach().numpy())\n            \n            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n            output = self.fcn(self.drop(h))\n            output = output.view(batch_size,-1)\n        \n            \n            \n            predicted_word_idx = output.argmax(dim=1)\n            \n            \n            captions.append(predicted_word_idx.item())\n            \n            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n                break\n            \n            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n        \n        return [vocab.itos[idx] for idx in captions],alphas\n    \n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  \n        c = self.init_c(mean_encoder_out)\n        return h, c","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:10.940401Z","iopub.execute_input":"2021-05-22T16:33:10.940804Z","iopub.status.idle":"2021-05-22T16:33:10.959512Z","shell.execute_reply.started":"2021-05-22T16:33:10.940769Z","shell.execute_reply":"2021-05-22T16:33:10.958672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        self.encoder = EncoderCNN()\n        self.decoder = DecoderRNN(embed_size=embed_size,vocab_size = len(dataset.vocab),attention_dim=attention_dim,encoder_dim=encoder_dim,decoder_dim=decoder_dim)\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:10.960822Z","iopub.execute_input":"2021-05-22T16:33:10.961315Z","iopub.status.idle":"2021-05-22T16:33:10.974302Z","shell.execute_reply.started":"2021-05-22T16:33:10.961278Z","shell.execute_reply":"2021-05-22T16:33:10.973514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:33:10.975497Z","iopub.execute_input":"2021-05-22T16:33:10.975840Z","iopub.status.idle":"2021-05-22T16:33:10.989829Z","shell.execute_reply.started":"2021-05-22T16:33:10.975806Z","shell.execute_reply":"2021-05-22T16:33:10.988890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nembed_size=300\nvocab_size = len(dataset.vocab)\nattention_dim=256\nencoder_dim=1024\ndecoder_dim=512\nlearning_rate = 3e-4","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:35:42.853702Z","iopub.execute_input":"2021-05-22T16:35:42.854034Z","iopub.status.idle":"2021-05-22T16:35:42.858592Z","shell.execute_reply.started":"2021-05-22T16:35:42.854003Z","shell.execute_reply":"2021-05-22T16:35:42.857538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel = EncoderDecoder(embed_size=300,vocab_size = len(dataset.vocab),attention_dim=256,encoder_dim=2048,decoder_dim=512).to(device)\ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:35:45.608766Z","iopub.execute_input":"2021-05-22T16:35:45.609115Z","iopub.status.idle":"2021-05-22T16:35:46.331157Z","shell.execute_reply.started":"2021-05-22T16:35:45.609084Z","shell.execute_reply":"2021-05-22T16:35:46.330283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1\nprint_every = 10\nimport time\nstart_time = time.time()\n\nfor epoch in range(1,num_epochs+1):   \n    for idx, (image, captions) in enumerate(iter(data_loader)):\n        print(idx)\n        image,captions = image.to(device),captions.to(device)\n\n        optimizer.zero_grad()\n        outputs,attentions = model(image, captions)\n\n        targets = captions[:,1:]\n        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n\n        loss.backward()\n\n        optimizer.step()\n\n        if (idx+1)%print_every == 0:\n            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))            \n            model.eval()\n            with torch.no_grad():\n                dataiter = iter(data_loader)\n                img,_ = next(dataiter)\n                features = model.encoder(img[0:1].to(device))\n                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n                caption = ' '.join(caps)\n                show_image(img[0],title=caption)                \n            model.train()        \nprint(\"--- %s seconds ---\" % (time.time() - start_time))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T16:35:50.832883Z","iopub.execute_input":"2021-05-22T16:35:50.833246Z","iopub.status.idle":"2021-05-22T19:09:25.100499Z","shell.execute_reply.started":"2021-05-22T16:35:50.833216Z","shell.execute_reply":"2021-05-22T19:09:25.099543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_caps_from(features_tensors):\n    model.eval()\n    with torch.no_grad():\n        features = model.encoder(features_tensors.to(device))\n        caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n        caption = ' '.join(caps)\n        show_image(features_tensors[0],title=caption)\n    \n    return caps,alphas\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:10:56.787760Z","iopub.execute_input":"2021-05-22T19:10:56.788122Z","iopub.status.idle":"2021-05-22T19:10:56.796955Z","shell.execute_reply.started":"2021-05-22T19:10:56.788090Z","shell.execute_reply":"2021-05-22T19:10:56.796148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def search(photo):\n    in_text='<start>'\n    sequence=[dataset.vocab.numericalize(s) for s in in_text.split(\" \") if dataset.vocab.frequencies[s]>=dataset.vocab.freq_threshold]\n    sequence = pad_sequences([sequence], maxlen=20, padding='post')\n    word,alpha=get_caps_from(photo)\n    return word","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:12:08.207514Z","iopub.execute_input":"2021-05-22T19:12:08.207838Z","iopub.status.idle":"2021-05-22T19:12:08.215639Z","shell.execute_reply.started":"2021-05-22T19:12:08.207805Z","shell.execute_reply":"2021-05-22T19:12:08.214708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nfrom nltk.translate.bleu_score import sentence_bleu\ntot_score=0\niterations=20\nfor idx in range(iterations):\n    print(idx)\n    caption = dataset.captions[idx]\n    img_name = dataset.imgs[idx]\n    img_location = os.path.join(dataset.root_dir,img_name)\n    img = Image.open(img_location).convert(\"RGB\")\n    img1=transforms.ToTensor()(img).unsqueeze_(0)\n    candidate=search(img1)\n    caption=[token.text.lower() for token in spacy_eng.tokenizer(caption)]\n    reference_list=[caption]\n    print(reference_list)\n    print(candidate)\n    score = sentence_bleu(reference_list,candidate)\n    tot_score+=score\n\n\navg=tot_score/iterations*1.0\nprint(avg)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:12:14.983206Z","iopub.execute_input":"2021-05-22T19:12:14.983532Z","iopub.status.idle":"2021-05-22T19:12:21.114395Z","shell.execute_reply.started":"2021-05-22T19:12:14.983503Z","shell.execute_reply":"2021-05-22T19:12:21.113607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img=Image.open('../input/vr-proj-images/sample5.jpg').convert(\"RGB\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:32:46.071280Z","iopub.execute_input":"2021-05-22T19:32:46.071596Z","iopub.status.idle":"2021-05-22T19:32:46.107918Z","shell.execute_reply.started":"2021-05-22T19:32:46.071568Z","shell.execute_reply":"2021-05-22T19:32:46.107144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nimg1=transforms.ToTensor()(img).unsqueeze_(0)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:33:53.745292Z","iopub.execute_input":"2021-05-22T19:33:53.745609Z","iopub.status.idle":"2021-05-22T19:33:53.766867Z","shell.execute_reply.started":"2021-05-22T19:33:53.745574Z","shell.execute_reply":"2021-05-22T19:33:53.766011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a,b=get_caps_from(img1)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:32:59.188733Z","iopub.execute_input":"2021-05-22T19:32:59.189071Z","iopub.status.idle":"2021-05-22T19:32:59.887436Z","shell.execute_reply.started":"2021-05-22T19:32:59.189043Z","shell.execute_reply":"2021-05-22T19:32:59.886676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_image(img1[0],title=a)","metadata":{"execution":{"iopub.status.busy":"2021-05-22T19:34:09.376980Z","iopub.execute_input":"2021-05-22T19:34:09.377300Z","iopub.status.idle":"2021-05-22T19:34:09.992300Z","shell.execute_reply.started":"2021-05-22T19:34:09.377271Z","shell.execute_reply":"2021-05-22T19:34:09.991464Z"},"trusted":true},"execution_count":null,"outputs":[]}]}